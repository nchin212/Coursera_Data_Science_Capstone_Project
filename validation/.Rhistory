dfCount1 <- fun.count(df1)
df2 <- data.frame(ngrams = train2)
dfCount2 <- fun.count(df2)
df3 <- data.frame(ngrams = train3)
dfCount3 <- fun.count(df3)
# Compute stupid backoff scores
dfTrain1 <- fun.score(dfCount1)
dfTrain2 <- fun.score(dfCount2,dfCount1) %>%
separate(ngrams, c('prevword', 'nextword'), " ")
dfTrain3 <- fun.score(dfCount3, dfCount2, words = 2) %>%
separate(ngrams, c('word1','word2','nextword'), " ")
# Profanity filtering
dirtySeven <- c('shit', 'piss', 'fuck', 'cunt', 'cocksucker', 'motherfucker', 'tits.')
dfTrain1 <- filter(dfTrain1, !nextword %in% dirtySeven)
dfTrain2 <- filter(dfTrain2, !prevword %in% dirtySeven &!nextword %in% dirtySeven)
dfTrain3 <- filter(dfTrain3, !word1 %in% dirtySeven & !word2 %in% dirtySeven & !nextword %in% dirtySeven)
dfTrain3$prevword <- paste(dfTrain3$word1,dfTrain3$word2)
dfTrain3 <- dfTrain3 %>% select(prevword,nextword,score)
# Save Dataframes
saveRDS(dfTrain1, file = 'dfTrain1.rds')
saveRDS(dfTrain2, file = 'dfTrain2.rds')
saveRDS(dfTrain3, file = 'dfTrain3.rds')
# Load libraries
source('./shiny/functions.R')
source('./data_cleaning.R')
# Input text sample
inputText <- 'kiss me'
# Get inputs as separate strings
input1 = fun.input(inputText)[1, ]
input2 = fun.input(inputText)[2, ]
input1
input2
# Predict
fun.predict(input1, input2, n=100)
# Input text sample
inputText <- 'why are'
# Get inputs as separate strings
input1 = fun.input(inputText)[1, ]
input2 = fun.input(inputText)[2, ]
input1
input2
# Predict
fun.predict(input1, input2, n=100)
# Input text sample
inputText <- 'park the'
# Get inputs as separate strings
input1 = fun.input(inputText)[1, ]
input2 = fun.input(inputText)[2, ]
input1
input2
# Predict
fun.predict(input1, input2, n=100)
# Input text sample
inputText <- 'what the'
# Get inputs as separate strings
input1 = fun.input(inputText)[1, ]
input2 = fun.input(inputText)[2, ]
input1
input2
# Predict
fun.predict(input1, input2, n=100)
# Input text sample
inputText <- 'who is'
# Get inputs as separate strings
input1 = fun.input(inputText)[1, ]
input2 = fun.input(inputText)[2, ]
input1
input2
# Predict
fun.predict(input1, input2, n=100)
source('data_cleaning.R')
source('shiny/functions.R')
# Create a quanteda corpus and reshape into sentences
valid = fun.corpus(valid)
# Get 2-gram tokens
valid2 = fun.tokenize(valid, 2)
valid2 = data_frame(ngrams = valid2) %>%
separate(ngrams, c('prevword', 'nextword'), ' ')
# Profanity filtering
dirtySeven <- read.table("List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.txt")
# Profanity filtering
dirtySeven <- read.delim("List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.txt")
dirtySeven
# Profanity filtering
dirtySeven <- readLines("List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.txt")
dirtySeven
str(dirtySeven)
# Profanity filtering
dirtySeven <- readLines("List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.txt")
dfTrain1 <- filter(dfTrain1, !nextword %in% dirtySeven)
dfTrain2 <- filter(dfTrain2, !prevword %in% dirtySeven &!nextword %in% dirtySeven)
dfTrain3 <- filter(dfTrain3, !word1 %in% dirtySeven & !word2 %in% dirtySeven & !nextword %in% dirtySeven)
# Combine back word1 and word2 in 3gram
dfTrain3$prevword <- paste(dfTrain3$word1,dfTrain3$word2)
dfTrain3 <- dfTrain3 %>% select(prevword,nextword,score)
# Convert character vectors into dataframe and compute counts of each ngram (prediction.R)
df1 <- data.frame(ngrams = train1)
dfCount1 <- fun.count(df1)
df2 <- data.frame(ngrams = train2)
dfCount2 <- fun.count(df2)
df3 <- data.frame(ngrams = train3)
dfCount3 <- fun.count(df3)
# Compute stupid backoff scores
dfTrain1 <- fun.score(dfCount1)
dfTrain2 <- fun.score(dfCount2,dfCount1) %>%
separate(ngrams, c('prevword', 'nextword'), " ")
dfTrain3 <- fun.score(dfCount3, dfCount2, words = 2) %>%
separate(ngrams, c('word1','word2','nextword'), " ")
# Profanity filtering
profanity <- readLines("List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words.txt")
dfTrain1 <- filter(dfTrain1, !nextword %in% profanity)
dfTrain2 <- filter(dfTrain2, !prevword %in% profanity &!nextword %in% profanity)
dfTrain3 <- filter(dfTrain3, !word1 %in% profanity & !word2 %in% profanity & !nextword %in% profanity)
# Combine back word1 and word2 in 3gram
dfTrain3$prevword <- paste(dfTrain3$word1,dfTrain3$word2)
dfTrain3 <- dfTrain3 %>% select(prevword,nextword,score)
# Save Dataframes
saveRDS(dfTrain1, file = 'dfTrain1.rds')
saveRDS(dfTrain2, file = 'dfTrain2.rds')
saveRDS(dfTrain3, file = 'dfTrain3.rds')
str(dfTrain3)
dfTrain3
# Create a quanteda corpus and reshape into sentences
valid <- fun.corpus(valid)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
head(valid2)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
head(valid3)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
fun.accu = function(x) {
# Apply prediction function to each input line
y = mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy = sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy percentage
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 3 suggestions
n_suggestions = 3
accuracy_1p_3s = round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 1 suggestion
n_suggestions = 1
accuracy_1p_1s = round(fun.accuracy(valid2), precision)
# Accuracy using 2 previous words and 5 suggestions
n_suggestions = 5
accuracy_2p_5s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 3 suggestions
n_suggestions = 3
accuracy_2p_3s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 1 suggestion
n_suggestions = 1
accuracy_2p_1s = round(fun.accuracy(valid3), precision)
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy percentage
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
valid
train
# Load functions in other file
source('data_cleaning.R')
source('shiny/functions.R')
set.seed(101)
n <- 1/10
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy percentage
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
valid2
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
set.seed(101)
n <- 1/100
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
# Create a accuracy function, where accuracy means percentage of cases where
# correct word is predicted within defined number of suggested words
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy percentage
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
set.seed(101)
n <- 1/1000
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
valid_sample
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
# Create a accuracy function, where accuracy means percentage of cases where
# correct word is predicted within defined number of suggested words
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy percentage
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
accuracy_1p_5s
# Accuracy using 1 previous word and 3 suggestions
n_suggestions = 3
accuracy_1p_3s = round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 1 suggestion
n_suggestions = 1
accuracy_1p_1s = round(fun.accuracy(valid2), precision)
# Accuracy using 2 previous words and 5 suggestions
n_suggestions = 5
accuracy_2p_5s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 3 suggestions
n_suggestions = 3
accuracy_2p_3s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 1 suggestion
n_suggestions = 1
accuracy_2p_1s = round(fun.accuracy(valid3), precision)
# Summary table
accuracy_table <- data.frame(Suggest5 = c(accuracy_2p_5s, accuracy_1p_5s),
Suggest3 = c(accuracy_2p_3s, accuracy_1p_3s),
Suggest1 = c(accuracy_2p_1s, accuracy_1p_1s),
row.names = c('Previous2', 'Previous1')
)
print(accuracy_table)
valid_sample
valid3
# Read in data
blogsRaw = read_lines('./data/en_US/en_US.blogs.txt')
newsRaw = read_lines('./data/en_US/en_US.news.txt')
twitterRaw = readLines('./data/en_US/en_US.twitter.txt') # Not working with readr because of an "embedded null"
combinedRaw = c(blogsRaw, newsRaw, twitterRaw)
# Read in data
blogsRaw = readLines('./data/en_US/en_US.blogs.txt')
newsRaw = readLines('./data/en_US/en_US.news.txt')
twitterRaw = readLines('./data/en_US/en_US.twitter.txt',skipNul = TRUE) # Not working with readr because of an "embedded null"
combinedRaw = c(blogsRaw, newsRaw, twitterRaw)
data.frame(lineCount = c(length(blogsRaw), length(newsRaw), length(twitterRaw), length(combinedRaw)),
medianNchar = c(median(nchar(blogsRaw)), median(nchar(newsRaw)), median(nchar(twitterRaw)), median(nchar(combinedRaw))),
row.names = c('Blogs', 'News', 'Twitter', 'Combined'))
min(nchar(combinedRaw))
median(nchar(combinedRaw))
max(nchar(combinedRaw))
valid_sample <- sample(valid, length(valid) * n)
# Load functions in other file
source('../data_cleaning/data_cleaning.R')
source('../shiny/functions.R')
set.seed(101)
n <- 1/1000
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
# Create a accuracy function, where accuracy means percentage of cases where
# correct word is predicted within defined number of suggested words
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 3 suggestions
n_suggestions = 3
accuracy_1p_3s = round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 1 suggestion
n_suggestions = 1
accuracy_1p_1s = round(fun.accuracy(valid2), precision)
# Accuracy using 2 previous words and 5 suggestions
n_suggestions = 5
accuracy_2p_5s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 3 suggestions
n_suggestions = 3
accuracy_2p_3s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 1 suggestion
n_suggestions = 1
accuracy_2p_1s = round(fun.accuracy(valid3), precision)
# Accuracy table
accuracy_table <- data.frame(Suggest5 = c(accuracy_2p_5s, accuracy_1p_5s),
Suggest3 = c(accuracy_2p_3s, accuracy_1p_3s),
Suggest1 = c(accuracy_2p_1s, accuracy_1p_1s),
row.names = c('Previous2', 'Previous1')
)
print(accuracy_table)
# Load functions in other file
source('../data_cleaning/data_cleaning.R')
# Load functions in other file
source('../data_cleaning/data_cleaning.R')
getwd()
setwd("/Users/nchin212/Desktop/Coursera_Data_Science_Capstone_Project/validation")
set.seed(101)
source('../data_cleaning/data_cleaning.R')
source('../shiny/functions.R')
set.seed(101)
n <- 1/1000
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
# Create a accuracy function, where accuracy means percentage of cases where
# correct word is predicted within defined number of suggested words
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 3 suggestions
n_suggestions = 3
accuracy_1p_3s = round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 1 suggestion
n_suggestions = 1
accuracy_1p_1s = round(fun.accuracy(valid2), precision)
# Accuracy using 2 previous words and 5 suggestions
n_suggestions = 5
accuracy_2p_5s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 3 suggestions
n_suggestions = 3
accuracy_2p_3s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 1 suggestion
n_suggestions = 1
accuracy_2p_1s = round(fun.accuracy(valid3), precision)
# Accuracy table
accuracy_table <- data.frame(Suggest5 = c(accuracy_2p_5s, accuracy_1p_5s),
Suggest3 = c(accuracy_2p_3s, accuracy_1p_3s),
Suggest1 = c(accuracy_2p_1s, accuracy_1p_1s),
row.names = c('Previous2', 'Previous1')
)
print(accuracy_table)
# Load functions in other file
source('../data_cleaning/data_cleaning.R')
setwd("/Users/nchin212/Desktop/Coursera_Data_Science_Capstone_Project/validation")
# Load functions in other file
source('../data_cleaning/data_cleaning.R')
# Load functions in other file
source('../data_cleaning/data_cleaning.rmd')
# Load functions in other files
source(source(knitr::purl("../data_cleaning/data_cleaning.rmd", quiet=TRUE))
# Load functions in other files
source(knitr::purl("../data_cleaning/data_cleaning.rmd", quiet=TRUE))
# Load functions in other files
source(knitr::purl("../data_cleaning/data_cleaning.rmd", quiet=TRUE))
source('../shiny/functions.R')
set.seed(101)
n <- 1/1000
valid_sample <- sample(valid, length(valid) * n)
# Create a quanteda corpus and reshape into sentences
valid_sample <- fun.corpus(valid_sample)
# Get 2-gram tokens
valid2 <- fun.tokenize(valid_sample, 2)
valid2 <- data_frame(ngrams = valid2) %>%
separate(ngrams, c('input2', 'nextword'), ' ')
# Put empty string as input1
valid2 <- mutate(valid2, input1 = rep("", nrow(valid2))) %>%
select(input1, input2, nextword)
# Get 3-gram tokens
valid3 <- fun.tokenize(valid_sample, 3)
valid3 <- data_frame(ngrams = valid3) %>%
separate(ngrams, c('input1', 'input2', 'nextword'), ' ')
# Create a accuracy function, where accuracy means percentage of cases where
# correct word is predicted within defined number of suggested words
fun.accuracy <- function(x) {
# Apply prediction function to each input line
y <- mapply(fun.predict, x$input1, x$input2)
# Calculate accuracy
accuracy <- sum(ifelse(x$nextword %in% unlist(y), 1, 0)) / nrow(x)
# Return accuracy
return(accuracy)
}
# Rounding precision
precision <- 2
# Accuracy using 1 previous word and 5 suggestions
n_suggestions <- 5
accuracy_1p_5s <- round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 3 suggestions
n_suggestions = 3
accuracy_1p_3s = round(fun.accuracy(valid2), precision)
# Accuracy using 1 previous word and 1 suggestion
n_suggestions = 1
accuracy_1p_1s = round(fun.accuracy(valid2), precision)
# Accuracy using 2 previous words and 5 suggestions
n_suggestions = 5
accuracy_2p_5s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 3 suggestions
n_suggestions = 3
accuracy_2p_3s = round(fun.accuracy(valid3), precision)
# Accuracy using 2 previous words and 1 suggestion
n_suggestions = 1
accuracy_2p_1s = round(fun.accuracy(valid3), precision)
# Accuracy table
accuracy_table <- data.frame(Suggest5 = c(accuracy_2p_5s, accuracy_1p_5s),
Suggest3 = c(accuracy_2p_3s, accuracy_1p_3s),
Suggest1 = c(accuracy_2p_1s, accuracy_1p_1s),
row.names = c('Previous2', 'Previous1')
)
print(accuracy_table)
